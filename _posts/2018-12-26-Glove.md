---
title: "Paper Review: GloVe - Global Vectors for Word Representation"
classes: wide
math: true
date: 2018-12-26
categories: NLP PaperReview
---

최근 주목받는 Word 또는 Character Embedding의 삼대장은 [Word2Vec](https://ws-choi.github.io/nlp/deeplearning/paperreview/Paper-Review-Distributed-Representations-ofWords-and-Phrases-and-their-Compositionality/), GloVe, FastText인 듯하다. [Ratsgo 블로그 포스트](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/11/embedding/)에서도 이 세 가지 임베딩을 집중적으로 다루고 있다. 그런데 최근 작성하고 있는 [포스트](https://ws-choi.github.io/nlp/deeplearning/paperreview/Recent-Trends-in-Deep-Learning-Based-Natural-Language/)에서 GloVe를 언급하기는 하는데 자세히 다루지는 않았다. 왜 3대장인데 언급한번하고 끝나지 하고 원논문을 읽어보았는데, 이유는 간단했다. **GloVe는 신경망 기반 기법이 아니기 때문.**

그렇다면 Distributed Representation을 만드는데 신경망을 기반으로 하지 않는 기법이 GloVe 이전에도 있었는가? 맞다. Global Matrix Factorization 기반의 Latent Semantic Analysis (LSA) 가 바로 그러한 예다. 오늘 다루고자 하는 논문은 Global Matrix Factorization 기법과 Word2Vec 등이 포함되는 Local Context Window 기법의 장점을 합친 기법인 **GloVe: Global Vectors for Words Representation 이라는 제목의 논문**이다. 

## 논문의 목적

[Word2Vec](https://ws-choi.github.io/nlp/deeplearning/paperreview/Paper-Review-Distributed-Representations-ofWords-and-Phrases-and-their-Compositionality/)는 여러가지 관점에서 혁신적인 논문이었다. 그 여러가지 혁신 중 이 논문에서 주목한 것은 Word2Vec의 벡터 산술 (Vector Arithmetic) 관련 성질이다. 

### 벡터 산술이란

개인적으로 벡터 산술은 Word2Vec이 그 이전의 기법보다 뛰어난 Distributed Representation 임을 아주 직관적으로 보여주는 증거라고 생각한다. Word2Vec가 보여주는 대표적인 벡터 산술의 예제를 보자.

$$word2vec(`` king") - word2vec(``man") + word2vec(``woman") \sim word2vec(``queen")$$

왕에서 남성적인 요소를 모두 지워버리고 여성적인 요소를 새로 부여하면 여왕의 개념과 부합한다는 점에서 설득력있는 Word Representation이다. 이 예제는 단어의 의미적 규칙 (semantic regularity)을 저차원 (e.g. 300차원) 실수 공간에서의 벡터 산술로 재현해냈다는 점에서 매우 큰 의의가 있다. 의미적 규칙을 산술연산으로 재현해내는 것은 기존의 Distributed Representation (e.g. LSA)에서는 불가능한 영역이었다. 

Word2Vec는 단어의 의미적 규칙외에도 문법적 규칙(syntactic regularity) 또한 재현해낼 수 있다. syntactic이 사전적으로 '통어적'이라고 하는데 말이 너무어려워서 '문법적'이라고 번역하겠다. 이제 아래 예제를 살펴보자.

$$word2vec(``dancing") - word2vec(``dance") + word2vec(``fly") \sim word2vec(``flying")$$

이번에는 dancing에서 dance를 빼고 fly를 더했다. dancing과 dance는 의미적으로 "춤추다"라는 뜻을 가지고 있는 것은 같으나 동명사냐 명사냐의 차이만을 가진다. 이 차이, 즉 동명사냐 명사냐의 차이를 동사인 fly에게 더해주면 flying이라는 결과가 나온다.  이 예제는 단어의 문법적 규칙이 저차원 실수 공간에서 벡터 산술로 재현해내었다는 것을 보여준다. 이 역시 기존의 Distributed Representation (e.g. LSA)에서는 불가능한 영역이었다. 

### 그래서 이 논문의 목적은

Word2Vec은 신기하게도 이러한 단어간의 규칙을 벡터 연산으로 표현하는 것에 성공했다. 그러나 저자진에 따르면 그 원리가 명확하게 규명된 적은 없었다고 한다. 

이 논문의 목적은 이러한 의미적, 문법적 규칙을 단어의 vector공간에서 재현하는 명확한 모델을 만들고 분석하는 것이다. 

## 기존 기법 및 문제

이 논문에서는 단어의 Distributed Representation을 위한 기존 기법을 두 가지로 나누어 분석하였다. 

(1) Matrix Factorization 

첫 번째는 LSA에서 파생된 Matrix Factorization Method 군이다. 이 기법들은 단어-문서 또는 단어-단어 Matrix를 만들어놓고 이 Matrix를 여러 개 ( e.g. 2개)의 행렬로 분해(Decomposition)한다. 이 과정에서 저차원의 공간에서의 단어 Distributed Representation이 가능하다. 이러한 기법은 단어에 대한 전체적인 (global) 통계정보를 활용한다는 점은 강점이나, **단어 유추 문제 (단어에서 나타나는 규칙성을 벡터 공간에서 재현할 수 있는가를 평가하는 문제)에 좋지 않은 성능을 보인다.** 

(2) Shallow Window-Based Method

(1)번 처럼 전체적인 단어 통계 정보를 이용하지는 않지만 지역적(local)인 문맥(context) 정보를 한정적으로 사용하여 단어를 vector 형태로 표현하는 방법이 있다. 우리가 이전에 다룬 [Word2Vec](https://ws-choi.github.io/nlp/deeplearning/paperreview/Paper-Review-Distributed-Representations-ofWords-and-Phrases-and-their-Compositionality/)의 경우도 한정된 window 크기 내의 단어가 주어졌을 때 중간 단어를 예측하는 과정이나 (CBoW: Continuous  Bag of Words) 반대로 중간 단어가 주어졌을 때 그 주변의 window 크기 내의 단어들을 예측하는 과정 (skip-gram)을 통해 단어를 벡터화하였다. 이 기법은 단어 유추 문제에서는 비교적 좋은 성능을 보이나 **학습 데이터(corpus)에서 관찰되는 단어사용 통계정보를 활용하지 않는 다는 점에서 한정적이다**. 이들은 지역적인 문맥에 대한 학습은 가능하지만, 학습 데이터(corpus)에서 관찰되는 서로 다른 두 단어의 동시발생정도 (co-occurrence)에 기반한 학습은 할 수 없기 때문이다. 

## 제안 기법

그렇다면 드는 의문은 이것이다.

>  Word2Vec가 충분히 단어에서 나타나는 규칙성을 잘 잡아내고 있지 않은가? Word2Vec이 단어사용 통계정보를 활용하지 않는게 문제라고 했는데, 그렇다면 단어사용 통계정보는 단어에서 나타나는 규칙성을 잡아내는 데에 기여할 수 있는가?

이 논문은 이 질문에서 시작한다. 이 질문에 답을 하기 위해 다음 예제를 살펴보자. 


| Probability and Ratio  | *k = solid*  |  *k = gas*  | *k = water* | *k = fashion* |
|---|---|---|---|---|
| P(k\|ice)| 1.9 x 10^-4 | 6.6 x 10^-5 | 3.0 x 10^-3 | 1.7 x 10^-4 |
| P(k\|steam)| 2.2 x 10^-5 | 7.8 x 10^-4 | 2.2 x 10^-3 | 1.8 x 10 ^-5 |
| P(k\|ice)/P(k\|steam) | 8.9 | 8.5 x 10 ^-2| 1.36| 0.96 |