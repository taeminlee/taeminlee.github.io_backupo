# Glove

최근 주목받는 Word 또는 Character Embedding의 삼대장은 [Word2Vec](https://ws-choi.github.io/nlp/deeplearning/paperreview/Paper-Review-Distributed-Representations-ofWords-and-Phrases-and-their-Compositionality/), GloVe, FastText인 듯하다. [Ratsgo 블로그 포스트](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/11/embedding/)에서도 이 세 가지 임베딩을 집중적으로 다루고 있다. 그런데 최근 작성하고 있는 [포스트](https://ws-choi.github.io/nlp/deeplearning/paperreview/Recent-Trends-in-Deep-Learning-Based-Natural-Language/)에서 GloVe를 언급하기는 하는데 자세히 다루지는 않았다. 왜 3대장인데 언급한번하고 끝나지 하고 원논문을 읽어보았는데, 이유는 간단했다. **GloVe는 신경망 기반 기법이 아니기 때문.**

그렇다면 Distributed Representation을 만드는데 신경망을 기반으로 하지 않는 기법이 GloVe 이전에도 있었는가? 맞다. Global Matrix Factorization 기반의 Latent Semantic Analysis (LSA) 가 바로 그러한 예다. 오늘 다루고자 하는 논문은 Global Matrix Factorization 기법과 Word2Vec 등이 포함되는 Local Context Window 기법의 장점을 합친 기법인 **GloVe: Global Vectors for Words Representation 이라는 제목의 논문**이다. 

## 논문의 목적

[Word2Vec](https://ws-choi.github.io/nlp/deeplearning/paperreview/Paper-Review-Distributed-Representations-ofWords-and-Phrases-and-their-Compositionality/)는 여러가지 관점에서 혁신적인 논문이었다. 그 여러가지 혁신 중 이 논문에서 주목한 것은 Word2Vec의 벡터 산술 (Vector Arithmetic) 관련 성질이다. 

### 벡터 산술이란

개인적으로 벡터 산술은 Word2Vec이 그 이전의 기법보다 뛰어난 Distributed Representation 임을 아주 직관적으로 보여주는 증거라고 생각한다. Word2Vec가 보여주는 대표적인 벡터 산술의 예제를 보자.

$$word2vec(`` king") - word2vec(``man") + word2vec(``woman") \sim word2vec(``queen")$$

왕에서 남성적인 요소를 모두 지워버리고 여성적인 요소를 새로 부여하면 여왕의 개념과 부합한다는 점에서 설득력있는 Word Representation이다. 이 예제는 단어의 의미적 규칙 (semantic regularity)을 저차원 (e.g. 300차원) 실수 공간에서의 벡터 산술로 재현해냈다는 점에서 매우 큰 의의가 있다. 의미적 규칙을 산술연산으로 재현해내는 것은 기존의 Distributed Representation (e.g. LSA)에서는 불가능한 영역이었다. 

Word2Vec는 단어의 의미적 규칙외에도 문법적 규칙(syntactic regularity) 또한 재현해낼 수 있다. syntactic이 사전적으로 '통어적'이라고 하는데 말이 너무어려워서 '문법적'이라고 번역하겠다. 이제 아래 예제를 살펴보자.

$$word2vec(``dancing") - word2vec(``dance") + word2vec(``fly") \sim word2vec(``flying")$$

이번에는 dancing에서 dance를 빼고 fly를 더했다. dancing과 dance는 의미적으로 "춤추다"라는 뜻을 가지고 있는 것은 같으나 동명사냐 명사냐의 차이만을 가진다. 이 차이, 즉 동명사냐 명사냐의 차이를 동사인 fly에게 더해주면 flying이라는 결과가 나온다.  이 예제는 단어의 문법적 규칙이 저차원 실수 공간에서 벡터 산술로 재현해내었다는 것을 보여준다. 이 역시 기존의 Distributed Representation (e.g. LSA)에서는 불가능한 영역이었다. 

Word2Vec은 신기하게도 이러한 단어간의 규칙을 벡터 연산으로 표현하는 것에 성공했다. 그러나 저자진에 따르면 그 원리가 명확하게 규명된 적은 없었다고 한다. 

### 그래서 이 논문의 목적은

이 논문의 목적은 이러한 의미적, 문법적 규칙을 단어의 vector공간에서 재현하는 명확한 모델을 만들고 분석하는 것이다. 

## 기존 기법 및 문제

## 해결 아이디어

## 제안 기법